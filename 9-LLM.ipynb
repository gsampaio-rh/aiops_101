{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. LLMs for Kubernetes Operations: Unlocking Insights from Logs and Metrics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the 9th notebook in our series on **AI for Kubernetes operations**! In this notebook, we dive into the transformative capabilities of **Large Language Models (LLMs)** and explore how they can enhance the way Kubernetes operators analyze and interpret complex operational data. \n",
    "\n",
    "By leveraging LLMs, operators can automate time-consuming tasks such as log analysis, incident summarization, and actionable recommendations, empowering them to focus on strategic decisions rather than repetitive, manual efforts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand the foundational principles behind **LLMs** and their transformer-based architecture.\n",
    "2. Explore how LLMs handle language processing tasks using **self-attention** and parallel processing.\n",
    "3. Interact with an LLM to perform tasks such as text generation, summarization, and classification.\n",
    "4. Use **RAG (Retrieval-Augmented Generation)** techniques.\n",
    "5. Combine **LLMs and RAG** workflows to extract actionable insights from complex data, showcasing how these tools can automate and simplify problem-solving.\n",
    "\n",
    "### What Are LLMs?\n",
    "\n",
    "**Large Language Models (LLMs)** are advanced AI systems trained on massive datasets of text to understand and generate human-like language. They excel at tasks such as answering questions, summarizing text, generating content, and even reasoning. Famous LLMs include:\n",
    "- **GPT**: A versatile model known for its fluency and wide range of capabilities.\n",
    "- **Claude**: A model optimized for safety and conversational clarity.\n",
    "- **DeepSeek**: Renowned for its precision in information retrieval and search-related tasks.\n",
    "- **LLaMA**: Lightweight and efficient, designed for fine-tuning on specific tasks.\n",
    "- **Gemini**: A cutting-edge model that combines multimodal understanding with language generation.\n",
    "\n",
    "LLMs are at the heart of modern AI applications because they can generalize across a wide range of domains and tasks with minimal additional training.\n",
    "\n",
    "![LLM Evolutionary Tree](https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.jpg?raw=true)\n",
    "\n",
    "<p><em>Source: Mooler0410, LLM Practical Guide</em></p>\n",
    "\n",
    "### Why Were LLMs Created?\n",
    "\n",
    "LLMs emerged to overcome limitations in earlier NLP models:\n",
    "1. **Contextual Understanding**:\n",
    "   - Models like RNNs and LSTMs struggled to grasp long-range dependencies in text. For example, they found it difficult to connect ideas across multiple sentences.\n",
    "2. **Training Inefficiency**:\n",
    "   - Sequential processing of input data made earlier models slow to train and scale.\n",
    "3. **Static Representations**:\n",
    "   - Traditional word embeddings (like Word2Vec) represented words without understanding their context, leading to ambiguity. For instance, the word \"bank\" could mean a financial institution or a riverbank.\n",
    "\n",
    "## Open Source and LLMs\n",
    "\n",
    "The term **open source** is widely used in the field of generative AI, but it often means different things depending on the model and context. In the world of **Large Language Models (LLMs)**, openness extends beyond simply releasing code or weights. It involves multiple aspects, such as transparency, accessibility, and documentation.\n",
    "\n",
    "### Dimensions of Openness in LLMs\n",
    "\n",
    "Openness in LLMs can be viewed as a **gradient** rather than a binary concept. Key dimensions of openness include:\n",
    "- **Model Weights**: Availability of the trained model weights for fine-tuning or deployment.\n",
    "- **Training Data Transparency**: Disclosure of the datasets used to train the model, ensuring reproducibility and fairness.\n",
    "- **Documentation**: The extent to which technical information, such as architecture details, preprints, and datasheets, is made available.\n",
    "- **Licensing and Access**: Whether the model is freely usable under open licenses and how accessible it is (e.g., via APIs or downloadable packages).\n",
    "\n",
    "### Levels of Openness\n",
    "\n",
    "Not all models claiming to be open source are truly open across all dimensions:\n",
    "- **Fully Open**: Models that release their weights, training data, and comprehensive documentation.\n",
    "- **Partially Open (\"Open Weight\")**: Models that release their weights but withhold details about the training data or fine-tuning processes.\n",
    "- **Closed**: Proprietary models that only provide access via APIs or under restrictive licenses.\n",
    "\n",
    "![Generative AI Openness Table](https://media.licdn.com/dms/image/v2/D4D22AQGMtO3uYxBJ0Q/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1690891092800?e=1740614400&v=beta&t=RKa5tJSEuu46Yh7fwumbmslui8q-iwdy6EZMUxNJk2c)\n",
    "\n",
    "<p><em>Source: <a href=\"https://pure.mpg.de/rest/items/item_3588217_2/component/file_3588218/content\" target=\"_blank\">\"Rethinking Open Source Generative AI\"</a></em></p>\n",
    "\n",
    "## First Interaction with an LLM\n",
    "\n",
    "Now that we’ve explored the foundational concepts behind Large Language Models (LLMs), let’s see them in action. In this section, we’ll interact with an LLM via an endpoint using a simple prompt.\n",
    "\n",
    "### Example: Asking the LLM to Explain Kubernetes\n",
    "\n",
    "We’ll send a straightforward request to the LLM to demonstrate its ability to generate clear and concise responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the Required Libraries\n",
    "\n",
    "Before we start, we need to install the necessary libraries. These include **transformers**, **torch**, and **scikit-learn**, which are required to build and fine-tune the BERT model. Run the following cell to install these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests langchain faiss-cpu sentence-transformers langchain_community langchain-ollama neo4j tqdm langchainhub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preparing the Request\n",
    "First, we define the system and user prompts to guide the model’s behavior and input.\n",
    "\n",
    "### What is a Prompt?\n",
    "\n",
    "A prompt is the input we give to an LLM to guide its response. It consists of two main components:\n",
    "\n",
    "- **System Prompt:** This defines the role, tone, and behavior of the model. It acts as a set of instructions or rules for how the LLM should respond. The system prompt sets the stage for the interaction by shaping the model's personality or context. For example, you can instruct the model to act as a teacher, assistant, or subject matter expert.\n",
    "\n",
    "  - *Example:* \"You are a helpful assistant that provides concise and factual answers to technical questions.\" \n",
    "\n",
    "- **User Prompt:** This is the actual input or question provided by the user. It is typically the main request or query for which the user seeks an answer or action. The quality of the user prompt is key, as clear and specific questions yield more accurate and relevant responses from the model.\n",
    "\n",
    "   - *Example:* \"What are the key features of Large Language Models?\"\n",
    "\n",
    "The way you craft your prompts significantly influences the quality and relevance of the LLM’s response.\n",
    "\n",
    "### 2.1. Crafting the Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(system_message: str, user_message: str) -> dict:\n",
    "    sys_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_message}<|eot_id|>\"\n",
    "    user_prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_message}<|eot_id|>\"\n",
    "    )\n",
    "\n",
    "    return {\"system\": sys_prompt, \"user\": user_prompt}\n",
    "\n",
    "prompts = create_prompt(\n",
    "    system_message=\"You are an IT operations assistant. Provide concise and actionable recommendations based on log data and system metrics.\",\n",
    "    user_message=\"Summarize the last 10 critical errors in the Kubernetes logs and suggest potential fixes.\",\n",
    ")\n",
    "\n",
    "print(\"System Prompt:\", prompts[\"system\"])\n",
    "print(\"User Prompt:\", prompts[\"user\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_payload(user_prompt: str, sys_prompt: str, model=\"llama3.1:latest\", temperature=0.0, stop=None):\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"prompt\": user_prompt,\n",
    "        \"system\": sys_prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the payload\n",
    "payload = prepare_payload(user_prompt=prompts[\"user\"], sys_prompt=prompts[\"system\"])\n",
    "print(\"Prepared payload:\", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters in LLM Interaction\n",
    "\n",
    "1. **Model**:\n",
    "   - Specifies the version of the LLM to be used.\n",
    "   - Example: `\"llama3.1:latest\"`, `\"gpt-4\"`.\n",
    "\n",
    "2. **Temperature**:\n",
    "   - Controls the randomness or creativity of the response.\n",
    "     - **Low values (e.g., 0.2)**: Generate more predictable and deterministic answers.\n",
    "     - **High values (e.g., 0.8)**: Produce more creative and diverse outputs.\n",
    "\n",
    "3. **Stop Sequences**:\n",
    "   - Specifies patterns that indicate where the model should stop generating.\n",
    "   - Example: `[“\\n”]` ensures the model stops at the end of a line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sending the Request\n",
    "\n",
    "Now that we have prepared the payload, let’s send it to the LLM endpoint and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Endpoint for the LLM\n",
    "endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(\n",
    "    endpoint, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "llm_response = response.json()\n",
    "print(\"LLM Response:\", llm_response.get(\"response\", \"No response received\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Interaction with Internal Processes\n",
    "\n",
    "This diagram represents the flow of interaction with a Large Language Model (LLM), including both the external and internal processes involved when sending a request and receiving a response.\n",
    "\n",
    "![image](images/llm_flow.png)\n",
    "\n",
    "1. **Define Request**: The user defines their request to the LLM.\n",
    "2. **Payload Preparation**\n",
    "\t* **System Prompt**: Sets the role or behavior of the LLM.\n",
    "\t* **User Prompt**: Specifies the question or task to be solved.\n",
    "\t* **Parameters**: Additional settings for fine-tuning the response (e.g., temperature, stop sequences).\n",
    "3. **Send Request to LLM**: The prepared payload is sent to the LLM for processing.\n",
    "4. **LLM Internal Processing**\n",
    "\t* **Tokenization**: Breaks down input into smaller parts (tokens).\n",
    "\t* **Inference/Computation**: Computes a response based on input tokens.\n",
    "\t* **Detokenization**: Converts output tokens back to human-readable text.\n",
    "\t* **Post-Processing**: Makes final adjustments based on parameters.\n",
    "5. **Receive & Process Response**: The LLM generates and sends the response.\n",
    "6. **Output Result**: The processed response is displayed to the user.\n",
    "7. **End**: The process concludes once the result is delivered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning the Parameters\n",
    "\n",
    "To refine the model's responses, you can adjust the following parameters in the payload:\n",
    "\n",
    "- **Temperature**: Controls the variability of the response.\n",
    "  - Low values (e.g., `0.2`) produce deterministic answers.\n",
    "  - High values (e.g., `0.8`) encourage creative and varied outputs.\n",
    "- **Max Tokens**: Limits the length of the response to prevent overly long outputs.\n",
    "- **Stop Sequences**: Defines when the LLM should stop generating text, useful for structured outputs.\n",
    "\n",
    "Let’s experiment with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with parameters\n",
    "payload = prepare_payload(\n",
    "    user_prompt=prompts[\"user\"], sys_prompt=prompts[\"system\"], temperature=0.8\n",
    ")\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(\n",
    "    endpoint, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Parse and display the response\n",
    "llm_response = response.json()\n",
    "print(\"Modified LLM Response:\", llm_response.get(\"response\", \"No response received\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What Is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that combines the capabilities of Large Language Models (LLMs) with external knowledge sources to enhance responses. By retrieving relevant information from structured or unstructured data, RAG enables LLMs to:\n",
    "- Provide more accurate, context-aware answers.\n",
    "- Overcome limitations of static knowledge (e.g., missing recent events or domain-specific details).\n",
    "- Handle large datasets without the need for retraining.\n",
    "\n",
    "### How RAG Works\n",
    "1. **Retrieval**:\n",
    "   - Extract relevant information from a knowledge source, such as:\n",
    "     - Graph databases (e.g., service dependencies, team assignments).\n",
    "     - Structured files (e.g., CSVs for logs or metrics).\n",
    "     - Unstructured documents (e.g., markdown files for incident reports).\n",
    "2. **Augmentation**:\n",
    "   - Combine the retrieved information with the user prompt to provide additional context.\n",
    "3. **Generation**:\n",
    "   - Use the LLM to generate a response that incorporates the augmented context.\n",
    "\n",
    "### Why Use RAG for Kubernetes Operations?\n",
    "\n",
    "In Kubernetes environments, operators deal with vast amounts of data from diverse sources. RAG can help:\n",
    "- **Log Analysis**: Retrieve logs matching specific error codes or timestamps and summarize issues.\n",
    "- **Service Dependencies**: Query graphs to identify which services might be impacted by a failing node.\n",
    "- **Configuration Documentation**: Retrieve markdown snippets describing configuration policies to answer questions like, “What is the resource limit for Pod X?”\n",
    "\n",
    "RAG transforms the LLM into a dynamic assistant that can access real-time, domain-specific knowledge, making it far more effective for IT operations.\n",
    "\n",
    "```plaintext\n",
    "          User Query\n",
    "              ↓\n",
    "       Data Retrieval\n",
    "    (e.g., Graph, CSV, Markdown)\n",
    "              ↓\n",
    "       Data Augmentation\n",
    "       (Combine Query + Data)\n",
    "              ↓\n",
    "     LLM Processes Input\n",
    "              ↓\n",
    "       Contextual Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings and Querying Markdown Files\n",
    "\n",
    "### What Are Embeddings?\n",
    "Before we dive into querying markdown files, it's essential to understand how embeddings work. \n",
    "\n",
    "Embeddings are dense vector representations of text data. They map text into a numerical space where similar pieces of text are located close together. Think of embeddings as the \"GPS coordinates\" of text in a high-dimensional space.\n",
    "  \n",
    "- **Why Use Embeddings?**\n",
    "  In Kubernetes operations, embeddings help in:\n",
    "  - **Log Retrieval**: Finding similar log entries for faster troubleshooting.\n",
    "  - **Configuration Matching**: Retrieving relevant sections of operational policies or resource limits.\n",
    "  - **Incident Analysis**: Locating similar incidents from past reports to guide current remediation.\n",
    "\n",
    "### 6.1. Creating Embeddings from Markdown Files\n",
    "\n",
    "1. **Loading Markdown Files**:\n",
    "  Use the `langchain` library to load and preprocess markdown files, splitting them into manageable chunks. Each chunk represents a meaningful section of the document, ensuring the context is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load Markdown files\n",
    "markdown_dir = \"./markdown_files\"  # Directory containing markdown files\n",
    "documents = []\n",
    "for filename in os.listdir(markdown_dir):\n",
    "    if filename.endswith(\".md\"):  # Ensure only Markdown files are loaded\n",
    "        loader = TextLoader(\n",
    "            os.path.join(markdown_dir, filename)\n",
    "        )  # Use TextLoader for reading\n",
    "        documents.extend(loader.load())  # Load and append each document\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Step 2: Split the content of documents into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # Maximum size of each chunk\n",
    "    chunk_overlap=100,  # Overlap between chunks for better context\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "split_documents = []\n",
    "for doc in documents:\n",
    "    chunks = splitter.split_text(doc.page_content)  # Split text into chunks\n",
    "    split_documents.extend(\n",
    "        [Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks]\n",
    "    )\n",
    "\n",
    "print(f\"Split into {len(split_documents)} chunks.\")\n",
    "\n",
    "# Verify the split documents\n",
    "for split_doc in split_documents[:5]:  # Display the first 5 chunks for verification\n",
    "    print(f\"Chunk metadata: {split_doc.metadata}\")\n",
    "    print(\n",
    "        f\"Chunk content:\\n{split_doc.page_content[:500]}\"\n",
    "    )  # Display the first 500 characters\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Generating Embeddings\n",
    "\n",
    "Use embeddings to convert text chunks into dense vectors that represent their semantic meaning.\n",
    "\n",
    "1. **Choosing an Embedding Model**:\n",
    "   Here, we use the `OllamaEmbeddings` module for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Define embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "\n",
    "# Generate embeddings\n",
    "vector_store = FAISS.from_documents(split_documents, embedding_model)\n",
    "print(f\"Vector store created with {vector_store.index.ntotal} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Querying Markdown Files with RAG\n",
    "\n",
    "1. **Defining the Retrieval Workflow**:\n",
    "   A retriever helps locate relevant chunks of text based on a user's query.\n",
    "\n",
    "2. **Creating the Prompt**:\n",
    "   Combine retrieved context with user queries to ensure the LLM generates accurate and contextual responses.\n",
    "\n",
    "3. **Integrating the LLM**:\n",
    "   Use a local model (e.g., `llama3.1`) to generate responses based on the augmented context.\n",
    "\n",
    "4. **Building the Full Chain**:\n",
    "   Combine the retriever, prompt, and LLM into a processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model_local = ChatOllama(model=\"llama3.1\", temperature=0.0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Example Query\n",
    "\n",
    "Query the vector store to retrieve relevant sections from markdown files, then use the LLM to generate a contextual response.\n",
    "\n",
    "1. **Query Example**:\n",
    "   Retrieve insights about a Kubernetes service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"What mechanism does StockTraderX use for low-latency order processing?\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
